{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe7e321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # only potentially necessary if you have MacBook with M1/M2 chip\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4487d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download MNIST data\n",
    "\n",
    "# download training set\n",
    "mnist_training = torchvision.datasets.MNIST('./', train=True, download=True, transform=transforms.ToTensor())\n",
    "# download test set\n",
    "mnist_testing = torchvision.datasets.MNIST('./', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a53122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample datasets\n",
    "N_training_examples = 5000 # roughly 500 per class\n",
    "N_validation_examples = 1000 # roughly 100 per class\n",
    "# grab indices of training and validation examples\n",
    "all_indices = np.random.choice(np.arange(len(mnist_training)),\n",
    "                               size=N_training_examples+N_validation_examples)\n",
    "training_indices = all_indices[:N_training_examples]\n",
    "validation_indices = all_indices[N_training_examples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef5b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model code\n",
    "class FullyConnected(nn.Module):\n",
    "    # constructor\n",
    "    def __init__(self, layer_widths=[784, 10], nonlinearity=nn.ReLU()):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(layer_widths)-2):\n",
    "            layers.append(nn.Linear(layer_widths[i], layer_widths[i+1])) # fully connected layer\n",
    "            layers.append(nonlinearity) # non-linearity\n",
    "        # add final layer\n",
    "        layers.append(nn.Linear(layer_widths[-2], layer_widths[-1]))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, x):\n",
    "        # input is passed in series to each module in the nn.Sequential object, i.e. each of the layers\n",
    "        return self.model(x)\n",
    "\n",
    "# example model with one hidden layer, hyperbolic tangent non-linearity\n",
    "model = FullyConnected([784, 100, 10], nn.Tanh())\n",
    "print(model.model)\n",
    "\n",
    "# example model with two hidden layers, ReLU non-linearity\n",
    "model = FullyConnected([784, 128, 64, 10])\n",
    "print(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb6a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_data, training_indices, validation_indices, config, verbose=False):\n",
    "    # unpack configuration parameters\n",
    "    lr = config['lr'] # learning rate\n",
    "    n_epochs = config['n_epochs'] # number of passes (epochs) through the training data\n",
    "    batch_size = config['batch_size']\n",
    "    \n",
    "    # set up optimizer and loss function\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss() # cross-entropy loss\n",
    "    \n",
    "    # set up dataloaders\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(training_indices)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(validation_indices)\n",
    "    trainloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, sampler=train_sampler)\n",
    "    valloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, sampler=val_sampler)\n",
    "    \n",
    "    # training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    best_val_accuracy = 0\n",
    "    best_model = None\n",
    "    for n in range(n_epochs):\n",
    "        # set model to training mode (unnecessary for this model, but good practice)\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for images, targets in trainloader:\n",
    "            images = images.view(-1, 784) # flatten images\n",
    "            optimizer.zero_grad() # zero out gradients\n",
    "            class_logits = model(images)\n",
    "            loss = criterion(class_logits, targets)\n",
    "            loss.backward() # backpropagate to compute gradients\n",
    "            optimizer.step() # update parameters using stochastic gradient descent\n",
    "            # update epoch statistics\n",
    "            epoch_loss += loss.item() # batch loss\n",
    "            epoch_acc += (class_logits.data.max(1)[1]).eq(targets).sum().item() # number of correct predictions\n",
    "            \n",
    "        # validation\n",
    "        epoch_loss /= len(trainloader)\n",
    "        epoch_acc /= len(training_indices)\n",
    "        val_loss, val_acc = validate(model, valloader, criterion)\n",
    "        val_loss /= len(valloader)\n",
    "        val_acc /= len(validation_indices)\n",
    "        \n",
    "        # log epoch information\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        # save best model, if necessary\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_model = copy.deepcopy(model)\n",
    "        if verbose and (n+1) % (int(n_epochs/20)) == 0:\n",
    "            print('Epoch {}/{}: (Train) Loss = {:.4e}, Acc = {:.4f}, (Val) Loss = {:.4e}, Acc = {:.4f}'.format(\n",
    "                   n+1,\n",
    "                   n_epochs,\n",
    "                   epoch_loss,\n",
    "                   epoch_acc,\n",
    "                   val_loss,\n",
    "                   val_acc))\n",
    "        \n",
    "        \n",
    "    return (np.array(train_losses),\n",
    "            np.array(train_accuracies),\n",
    "            np.array(val_losses),\n",
    "            np.array(val_accuracies),\n",
    "            best_model)\n",
    "        \n",
    "def validate(model, dataloader, criterion):\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    # set model to eval mode (again, unnecessary here but good practice)\n",
    "    model.eval()\n",
    "    # don't compute gradients since we are not updating the model, saves a lot of computation\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = images.view(-1, 784)\n",
    "            class_logits = model(images)\n",
    "            loss = criterion(class_logits, targets)\n",
    "            val_loss += loss.item()\n",
    "            val_acc += (class_logits.data.max(1)[1]).eq(targets).sum().item()\n",
    "    return val_loss, val_acc\n",
    "\n",
    "def test(model, dataset):\n",
    "    # test best model on withheld test data\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=128)\n",
    "    test_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = images.view(-1, 784)\n",
    "            class_logits = model(images)\n",
    "            test_acc += (class_logits.data.max(1)[1]).eq(targets).sum().item()\n",
    "    return test_acc/len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0441be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting it all together\n",
    "\n",
    "# data\n",
    "N_training_examples = 1000 \n",
    "N_validation_examples = 1000 # 100 examples per class\n",
    "random_seed = 1 # random seed for reproducibility\n",
    "np.random.seed(random_seed)\n",
    "all_indices = np.random.choice(np.arange(len(mnist_training)),\n",
    "                               size=N_training_examples+N_validation_examples)\n",
    "training_indices = all_indices[:N_training_examples]\n",
    "validation_indices = all_indices[N_training_examples:]\n",
    "\n",
    "# model\n",
    "layer_widths = [784, 100, 10] # one hidden layer\n",
    "nonlinearity = nn.ReLU() # ReLU nonlinearity\n",
    "model = FullyConnected(layer_widths=layer_widths, nonlinearity=nonlinearity)\n",
    "\n",
    "# configuration parameters\n",
    "config = {'lr': 1e-1,\n",
    "          'n_epochs': 200,\n",
    "          'batch_size': 128}\n",
    "\n",
    "# train\n",
    "verbose = True\n",
    "train_losses, train_accs, val_losses, val_accs, best_model = train(model,\n",
    "                                                                   mnist_training,\n",
    "                                                                   training_indices,\n",
    "                                                                   validation_indices,\n",
    "                                                                   config,\n",
    "                                                                   verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5ab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect training results\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(221)\n",
    "plt.semilogy(train_losses, color='royalblue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Training loss')\n",
    "plt.grid(True)\n",
    "plt.subplot(222)\n",
    "plt.plot(train_accs, color='darkorange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Training accuracy')\n",
    "plt.grid(True)\n",
    "plt.subplot(223)\n",
    "plt.plot(val_losses, color='royalblue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Validation loss')\n",
    "plt.grid(True)\n",
    "plt.subplot(224)\n",
    "plt.plot(val_accs, color='darkorange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Validation accuracy')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b68f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test best model\n",
    "print('Test accuracy = {:.4f}'.format(test(best_model, mnist_testing)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe7e321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # only potentially necessary if you have MacBook with M1/M2 chip\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4487d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download MNIST data\n",
    "\n",
    "# download training set\n",
    "mnist_training = torchvision.datasets.MNIST('./', train=True, download=True, transform=transforms.ToTensor())\n",
    "# download test set\n",
    "mnist_testing = torchvision.datasets.MNIST('./', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef5b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model code\n",
    "class CNN(nn.Module):\n",
    "    # constructor\n",
    "    def __init__(self, channel_widths, pooling, nonlinearity=nn.ReLU()):\n",
    "        super(CNN, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(channel_widths)-2):\n",
    "            # convolution layer\n",
    "            # you can play around with kernel_size, padding, and stride if you like\n",
    "            # kernel_size is most likely to have an impact\n",
    "            # arguments are: Conv2d(n_input_channels, n_output_channels, \n",
    "            #                       filter_side_length, padding, stride_length)\n",
    "            layers.append(nn.Conv2d(channel_widths[i], channel_widths[i+1],\n",
    "                                    kernel_size=5, padding=2, stride=1, bias=False))\n",
    "            layers.append(nonlinearity) # non-linearity\n",
    "        # add final layer\n",
    "        layers.append(nn.Conv2d(channel_widths[-2], channel_widths[-1],\n",
    "                                    kernel_size=5, padding=2, stride=1, bias=False))\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "        self.global_pooling = pooling # reduce each of the H x W feature maps to a single pooled value\n",
    "        self.pool_size = pooling.output_size[0]*pooling.output_size[1]\n",
    "        self.linear = nn.Linear(channel_widths[-1]*self.pool_size, 10)  # score each class to obtain logits\n",
    "    # forward pass\n",
    "    def forward(self, x):\n",
    "        B = x.size(0) # number of input images\n",
    "        features = self.backbone(x) # get feature maps (B, N_feature_maps, H, W)\n",
    "        pooled_features = self.global_pooling(features) # (B, N_feature_maps, 1, 1)\n",
    "        pooled_features = pooled_features.view(B, -1) # (B, N_feature_maps)\n",
    "        logits = self.linear(pooled_features) # (B, N_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb6a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_data, training_indices, validation_indices, config, verbose=False):\n",
    "    # unpack configuration parameters\n",
    "    lr = config['lr'] # learning rate\n",
    "    n_epochs = config['n_epochs'] # number of passes (epochs) through the training data\n",
    "    batch_size = config['batch_size']\n",
    "    \n",
    "    # set up optimizer and loss function\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # set up dataloaders\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(training_indices)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(validation_indices)\n",
    "    trainloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, sampler=train_sampler)\n",
    "    valloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, sampler=val_sampler)\n",
    "    \n",
    "    # training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    best_val_accuracy = 0\n",
    "    best_model = None\n",
    "    for n in range(n_epochs):\n",
    "        # set model to training mode (unnecessary for this model, but good practice)\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for images, targets in trainloader:\n",
    "            optimizer.zero_grad() # zero out gradients\n",
    "            class_logits = model(images)\n",
    "            loss = criterion(class_logits, targets)\n",
    "            loss.backward() # backpropagate to compute gradients\n",
    "            optimizer.step() # update parameters using stochastic gradient descent\n",
    "            # update epoch statistics\n",
    "            epoch_loss += loss.item() # batch loss\n",
    "            epoch_acc += (class_logits.data.max(1)[1]).eq(targets).sum().item() # number of correct predictions\n",
    "            \n",
    "        # validation\n",
    "        epoch_loss /= len(trainloader)\n",
    "        epoch_acc /= len(training_indices)\n",
    "        val_loss, val_acc = validate(model, valloader, criterion)\n",
    "        val_loss /= len(valloader)\n",
    "        val_acc /= len(validation_indices)\n",
    "        \n",
    "        # log epoch information\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        # save best model, if necessary\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_model = copy.deepcopy(model)\n",
    "        if verbose and (n+1) % (int(n_epochs/20)) == 0:\n",
    "            print('Epoch {}/{}: (Train) Loss = {:.4e}, Acc = {:.4f}, (Val) Loss = {:.4e}, Acc = {:.4f}'.format(\n",
    "                   n+1,\n",
    "                   n_epochs,\n",
    "                   epoch_loss,\n",
    "                   epoch_acc,\n",
    "                   val_loss,\n",
    "                   val_acc))\n",
    "        \n",
    "        \n",
    "    return (np.array(train_losses),\n",
    "            np.array(train_accuracies),\n",
    "            np.array(val_losses),\n",
    "            np.array(val_accuracies),\n",
    "            best_model)\n",
    "        \n",
    "def validate(model, dataloader, criterion):\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    # set model to eval mode (again, unnecessary here but good practice)\n",
    "    model.eval()\n",
    "    # don't compute gradients since we are not updating the model, saves a lot of computation\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            class_logits = model(images)\n",
    "            loss = criterion(class_logits, targets)\n",
    "            val_loss += loss.item()\n",
    "            val_acc += (class_logits.data.max(1)[1]).eq(targets).sum().item()\n",
    "    return val_loss, val_acc\n",
    "\n",
    "def test(model, dataset):\n",
    "    # test best model on withheld test data\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=128)\n",
    "    test_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            class_logits = model(images)\n",
    "            test_acc += (class_logits.data.max(1)[1]).eq(targets).sum().item()\n",
    "    return test_acc/len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a489ff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics visualization\n",
    "\n",
    "# data\n",
    "# you can try playing around with more or less training data\n",
    "N_training_examples = 1000 # 100 examples per class\n",
    "N_validation_examples = 1000 # 100 examples per class\n",
    "random_seed = 1 # random seed for reproducibility\n",
    "np.random.seed(random_seed)\n",
    "all_indices = np.random.choice(np.arange(len(mnist_training)),\n",
    "                               size=N_training_examples+N_validation_examples)\n",
    "training_indices = all_indices[:N_training_examples]\n",
    "validation_indices = all_indices[N_training_examples:]\n",
    "\n",
    "# configuration parameters, you can play around with these\n",
    "config = {'lr': 3e-1,\n",
    "          'n_epochs': 200,\n",
    "          'batch_size': 100}\n",
    "pooling = 'max' # 'max' or 'mean'\n",
    "\n",
    "if pooling == 'max':\n",
    "    pool_fcn = nn.AdaptiveMaxPool2d((1, 1))\n",
    "else:\n",
    "    pool_fcn = nn.AdaptiveAvgPool2d((1, 1))\n",
    "# model\n",
    "# your model will have length(channel_widths)-1 layers\n",
    "channel_widths = [1, 16, 16] # must start with a 1 and be at least length--2\n",
    "model = CNN(channel_widths, pool_fcn)\n",
    "\n",
    "# train\n",
    "verbose = True # print metrics during training, False for no printing\n",
    "train_losses, train_accs, val_losses, val_accs, best_model = train(model,\n",
    "                                                                   mnist_training,\n",
    "                                                                   training_indices,\n",
    "                                                                   validation_indices,\n",
    "                                                                   config,\n",
    "                                                                   verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52edc183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training/validation loss and accuracy over training time\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(221)\n",
    "plt.semilogy(train_losses, color='royalblue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Training loss')\n",
    "plt.grid(True)\n",
    "plt.subplot(222)\n",
    "plt.plot(train_accs, color='darkorange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Training accuracy')\n",
    "plt.grid(True)\n",
    "plt.subplot(223)\n",
    "plt.plot(val_losses, color='royalblue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Validation loss')\n",
    "plt.grid(True)\n",
    "plt.subplot(224)\n",
    "plt.plot(val_accs, color='darkorange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Validation accuracy')\n",
    "plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
